# Barebones Neural Network (Leaky ReLU)

A simple **from-scratch Neural Network** implemented in **NumPy** that achieves **100% accuracy** on a toy classification dataset using **Leaky ReLU** activation.  
Perfect for **beginners** to understand forward pass, backpropagation, weight updates, and decision boundaries â€” **no frameworks required**.

---

## Features
- Fully implemented in **pure NumPy**
- Two-layer Neural Network
- **Leaky ReLU** activation for stable training
- **Softmax** output for multi-class classification
- Visualizations:
  - Training vs. Validation accuracy curves
  - Decision boundary plots
- Compatible with **Google Colab** and **local Python environments**

---
